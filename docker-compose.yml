
services:
  airflow-initdb:
    container_name: airflow-initdb
    build:
      context: ./build/airflow
    volumes:
      - ${AIRFLOW_CODE_PATH}:/opt/airflow
      - ${AIRFLOW_DAGS_PATH}:/opt/airflow-dags
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create -u ${AIRFLOW_ADMIN_USERNAME} -f ${AIRFLOW_ADMIN_FIRSTNAME} -l ${AIRFLOW_ADMIN_LASTNAME} -r Admin -e ${AIRFLOW_ADMIN_EMAIL} -p '${AIRFLOW_ADMIN_PASSWORD}'

  airflow-worker:
    build:
      context: ./build/airflow
    container_name: airflow-worker
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - ${AIRFLOW_CODE_PATH}:/opt/airflow
      - ${AIRFLOW_DAGS_PATH}:/opt/airflow-dags
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: False
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow-dags
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      airflow-initdb:
        condition: service_completed_successfully

  airflow-webserver:
    container_name: airflow-webserver
    build:
      context: ./build/airflow
    depends_on:
      - airflow-initdb
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: False
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow-dags
    volumes:
      - ${AIRFLOW_CODE_PATH}:/opt/airflow
      - ${AIRFLOW_DAGS_PATH}:/opt/airflow-dags
    ports:
      - ${AIRFLOW_WEBSERVER_PORT}:${AIRFLOW_WEBSERVER_PORT}
      - ${AIRFLOW_WEBSERVER_DEBUG_PORT}:${AIRFLOW_WEBSERVER_DEBUG_PORT}
    entrypoint: airflow webserver

  airflow-scheduler:
    container_name: airflow-scheduler
    build:
      context: ./build/airflow
    depends_on:
      - airflow-initdb
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES: False
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow-dags
    volumes:
      - ${AIRFLOW_CODE_PATH}:/opt/airflow
      - ${AIRFLOW_DAGS_PATH}:/opt/airflow-dags
    entrypoint: airflow scheduler

  redis:
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    container_name: redis
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  minio:
    container_name:  minio
    image: quay.io/minio/minio:latest
    command: server /data --address ":${MINIO_API_PORT}" --console-address ":${MINIO_CONSOLE_PORT}"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - ${MINIO_API_PORT}:${MINIO_API_PORT}
      - ${MINIO_CONSOLE_PORT}:${MINIO_CONSOLE_PORT}
    volumes:
      - ${MINIO_DATA_PATH}:/data                          # Persist data on host

  broker:
    image: apache/kafka:latest
    container_name: broker
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,BROKER://broker:9094,CONTROLLER://localhost:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,BROKER://broker:9094
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,BROKER:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    ports:
      - ${KAFKA_PORT}:${KAFKA_PORT}

  clickhouse:
    image: clickhouse/clickhouse-server
    container_name: clickhouse
    ports:
      - ${CLICKHOUSE_HTTP_PORT}:8123
      - ${CLICKHOUSE_TCP_PORT}:9000
    volumes:
      - ${CLICKHOUSE_DATA_PATH}:/var/lib/clickhouse
      - ${CLICKHOUSE_LOG_PATH}:/var/log/clickhouse-server
    environment:
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./services/postgres:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  superset:
    build:
      context: ./build/superset
    container_name: superset
    environment:
      ADMIN_USERNAME: ${SUPERSET_ADMIN_USERNAME}
      ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
      ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL}
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
    ports:
      - ${SUPERSET_PORT}:8088
    volumes:
      - ${SUPERSET_HOME_PATH}:/app/superset_home

  mlflow:
    container_name: mlflow
    image: ghcr.io/mlflow/mlflow:v2.15.1
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:${MINIO_API_PORT}
      AWS_ACCESS_KEY_ID: $MINIO_ROOT_USER
      AWS_SECRET_ACCESS_KEY: $MINIO_ROOT_PASSWORD
      AWS_DEFAULT_REGION: ru-central1
    command:
      - bash
      - -c
      - |
        pip install boto3 --quiet
        mlflow server --host 0.0.0.0 \
            --default-artifact-root s3://mlflow/mlflow --serve-artifacts
    ports:
      - 5050:5000
    volumes:
      - ${MLFLOW_DIR}:/mlflow

  jupyter:
    container_name: jupyter
    build:
      context: ./build/jupyter
    ports:
      - 8888:8888
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
      AWS_ENDPOINT_URL: http://minio:${MINIO_API_PORT}
      AWS_ACCESS_KEY_ID: $MINIO_ROOT_USER
      AWS_SECRET_ACCESS_KEY: $MINIO_ROOT_PASSWORD
      AWS_DEFAULT_REGION: ru-central1
    depends_on:
      - mlflow